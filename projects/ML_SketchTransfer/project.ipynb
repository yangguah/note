{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "FwbFS-f70SRA"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch #require torch version == 0.4.0\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "from torchvision import utils\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import os, os.path\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "plt.rcParams[\"axes.grid\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "sS2r13M40jqg"
   },
   "outputs": [],
   "source": [
    "#global vars\n",
    "std_output_channel=64\n",
    "std_batch_size=4\n",
    "gpu_enabled=torch.cuda.is_available()\n",
    "edge_threshold=0.2\n",
    "std_sketch_loss=nn.L1Loss()\n",
    "g_learning_rate=1e-5\n",
    "d_learning_rate=1e-5\n",
    "optim_betas = (0.9, 0.999)\n",
    "GP_lambda=0.5\n",
    "SL_lambda=0.5\n",
    "IL_lambda=0.5\n",
    "dtype=torch.cuda.FloatTensor if gpu_enabled else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "lDsVBk_f1h0D"
   },
   "outputs": [],
   "source": [
    "#preprocessing functions\n",
    "def LoadDataSet(path)->\"grayscale pictures\":\n",
    "    data_transform=T.Compose([\n",
    "        T.Resize((256,256)),\n",
    "        T.Grayscale(1),\n",
    "        T.ToTensor(),\n",
    "    ])\n",
    "    dataset = dset.ImageFolder(root=path,transform=data_transform)\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=std_batch_size,shuffle=True, num_workers=1,pin_memory=True)\n",
    "\n",
    "def LoadTestSet(path):\n",
    "    res=[]\n",
    "    for f in os.listdir(path):\n",
    "        img=Image.open(os.path.join(path,f))\n",
    "        w,h=img.size\n",
    "        w=64*round(w/64)\n",
    "        h=64*round(h/64)\n",
    "        transform=T.Compose([T.Resize((h,w)),T.Grayscale(1),T.ToTensor()])\n",
    "        res.append(transform(img).type(dtype))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "0G8H0Zv21p88"
   },
   "outputs": [],
   "source": [
    "#util functions/classes\n",
    "\n",
    "def ShowImage(inp):\n",
    "    inp = utils.make_grid(inp)\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    plt.imshow(inp)\n",
    "    plt.pause(0.01)\n",
    "    \n",
    "def SaveImage(path,img):\n",
    "    img = utils.make_grid(img)\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    plt.imsave(path,img)\n",
    "    \n",
    "def Tensor2Edges(img,mode='Canny')->\"edges\":\n",
    "    #cv2.Laplacian\n",
    "    #cv2.Sobel\n",
    "    #cv2.Canny\n",
    "    img=img.squeeze(1)\n",
    "    res=np.uint8(img.data*255)\n",
    "    for i in range(res.shape[0]):\n",
    "        if mode=='Canny':\n",
    "            res[i]=cv2.Canny(res[i],80,150)/255\n",
    "        elif mode=='Laplacian':\n",
    "            res[i]=cv2.Laplacian(res[i],cv2.CV_8U)/255\n",
    "        elif mode=='Sobel':\n",
    "            res[i]=cv2.Sobel(res[i],cv2.CV_8U,1,1,ksize=5)/255\n",
    "        else:\n",
    "            return None\n",
    "    return torch.tensor(1-res).unsqueeze(1).type(dtype)\n",
    "\n",
    "def SketchLoss(img,edges)->\"loss\":\n",
    "    assert(img.shape==edges.shape)\n",
    "    return torch.mean(torch.abs(img-edges)).type(dtype) #L1 loss\n",
    "\n",
    "def ProbLessthan(prob):\n",
    "    return random.random()<prob\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size() \n",
    "        return x.view(N, -1)\n",
    "\n",
    "def SaveModel(model,path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def LoadModel(model,path):\n",
    "    res=model().type(dtype)\n",
    "    res.load_state_dict(torch.load(path))\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "a7qOiR-51rsJ"
   },
   "outputs": [],
   "source": [
    "TestSet=LoadTestSet('TestSamples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "PPGcbJgL7W8d"
   },
   "outputs": [],
   "source": [
    "class UnetGenerator(nn.Module):\n",
    "    def __init__(self, input_nc=1, output_nc=1, num_downs=6, ngf=64,\n",
    "                 norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        super(UnetGenerator, self).__init__()\n",
    "\n",
    "        # construct unet structure\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)\n",
    "        for i in range(num_downs - 5):\n",
    "            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        unet_block = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)\n",
    "\n",
    "        self.model = unet_block\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "\n",
    "\n",
    "# Defines the submodule with skip connection.\n",
    "# X -------------------identity---------------------- X\n",
    "#   |-- downsampling -- |submodule| -- upsampling --|\n",
    "class UnetSkipConnectionBlock(nn.Module):\n",
    "    def __init__(self, outer_nc, inner_nc, input_nc=None,\n",
    "                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        super(UnetSkipConnectionBlock, self).__init__()\n",
    "        self.outermost = outermost\n",
    "        use_bias=False\n",
    "        if input_nc is None:\n",
    "            input_nc = outer_nc\n",
    "        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n",
    "                             stride=2, padding=1, bias=use_bias)\n",
    "        downrelu = nn.LeakyReLU(0.2, True)\n",
    "        downnorm = norm_layer(inner_nc)\n",
    "        uprelu = nn.ReLU(True)\n",
    "        upnorm = norm_layer(outer_nc)\n",
    "\n",
    "        if outermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1)\n",
    "            down = [downconv]\n",
    "            up = [uprelu, upconv, nn.Tanh()]\n",
    "            model = down + [submodule] + up\n",
    "        elif innermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, bias=use_bias)\n",
    "            down = [downrelu, downconv]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            model = down + up\n",
    "        else:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, bias=use_bias)\n",
    "            down = [downrelu, downconv, downnorm]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "\n",
    "            if use_dropout:\n",
    "                model = down + [submodule] + up + [nn.Dropout(0.5)]\n",
    "            else:\n",
    "                model = down + [submodule] + up\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.outermost:\n",
    "            return self.model(x)\n",
    "        else:\n",
    "            return torch.cat([x, self.model(x)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "AUoRFWat2oUl"
   },
   "outputs": [],
   "source": [
    "G_path=\"Models/G_Unet\" #can also download the Unet Generator model at https://drive.google.com/open?id=15NnVLJgSQmua-0DL2bf2xae4w56TDKbM\n",
    "G=LoadModel(UnetGenerator,G_path)\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "yYHxDVRK2xmo"
   },
   "outputs": [],
   "source": [
    "for i in TestSet:\n",
    "    with torch.no_grad():\n",
    "        testdata=i.unsqueeze(0).type(dtype)\n",
    "        patches=testdata*2-1\n",
    "        G_res=G(patches)\n",
    "        G_tensor=(G_res.detach()+1)/2\n",
    "        print('image:')\n",
    "        ShowImage(testdata.cpu())\n",
    "        print('generated sketch:')\n",
    "        ShowImage(G_tensor.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Vogc14ld7Y-1"
   },
   "outputs": [],
   "source": [
    "class ResnetGenerator(nn.Module):\n",
    "    def __init__(self, input_nc=1, output_nc=1, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, padding_type='reflect'):\n",
    "        assert(n_blocks >= 0)\n",
    "        super(ResnetGenerator, self).__init__()\n",
    "        self.input_nc = input_nc\n",
    "        self.output_nc = output_nc\n",
    "        self.ngf = ngf\n",
    "        use_bias=False\n",
    "\n",
    "        model = [nn.ReflectionPad2d(3),\n",
    "                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0,\n",
    "                           bias=use_bias),\n",
    "                 norm_layer(ngf),\n",
    "                 nn.ReLU(True)]\n",
    "\n",
    "        n_downsampling = 2\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2**i\n",
    "            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,\n",
    "                                stride=2, padding=1, bias=use_bias),\n",
    "                      norm_layer(ngf * mult * 2),\n",
    "                      nn.ReLU(True)]\n",
    "\n",
    "        mult = 2**n_downsampling\n",
    "        for i in range(n_blocks):\n",
    "            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n",
    "\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2**(n_downsampling - i)\n",
    "            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n",
    "                                         kernel_size=3, stride=2,\n",
    "                                         padding=1, output_padding=1,\n",
    "                                         bias=use_bias),\n",
    "                      norm_layer(int(ngf * mult / 2)),\n",
    "                      nn.ReLU(True)]\n",
    "        model += [nn.ReflectionPad2d(3)]\n",
    "        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n",
    "        model += [nn.Tanh()]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "\n",
    "\n",
    "# Define a resnet block\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n",
    "\n",
    "    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
    "        conv_block = []\n",
    "        p = 0\n",
    "        if padding_type == 'reflect':\n",
    "            conv_block += [nn.ReflectionPad2d(1)]\n",
    "        elif padding_type == 'replicate':\n",
    "            conv_block += [nn.ReplicationPad2d(1)]\n",
    "        elif padding_type == 'zero':\n",
    "            p = 1\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
    "                       norm_layer(dim),\n",
    "                       nn.ReLU(True)]\n",
    "        if use_dropout:\n",
    "            conv_block += [nn.Dropout(0.5)]\n",
    "\n",
    "        p = 0\n",
    "        if padding_type == 'reflect':\n",
    "            conv_block += [nn.ReflectionPad2d(1)]\n",
    "        elif padding_type == 'replicate':\n",
    "            conv_block += [nn.ReplicationPad2d(1)]\n",
    "        elif padding_type == 'zero':\n",
    "            p = 1\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
    "                       norm_layer(dim)]\n",
    "\n",
    "        return nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.conv_block(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "D0OGKhzI7bdJ"
   },
   "outputs": [],
   "source": [
    "G_path=\"Models/G_Resnet\" #can also download the Resnet Generator model at https://drive.google.com/open?id=15NnVLJgSQmua-0DL2bf2xae4w56TDKbM\n",
    "G=LoadModel(ResnetGenerator,G_path)\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "5U_w4_PF7fXz"
   },
   "outputs": [],
   "source": [
    "for i in TestSet:\n",
    "    with torch.no_grad():\n",
    "        testdata=i.unsqueeze(0).type(dtype)\n",
    "        patches=testdata*2-1\n",
    "        G_res=G(patches)\n",
    "        G_tensor=(G_res.detach()+1)/2\n",
    "        print('image:')\n",
    "        ShowImage(testdata.cpu())\n",
    "        print('generated sketch:')\n",
    "        ShowImage(G_tensor.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "92IXsYYy7jyC"
   },
   "outputs": [],
   "source": [
    "class Generator_Layer(nn.Module):\n",
    "    def __init__(self,in_c,out_c,relu_layer=nn.ELU,norm_layer=nn.BatchNorm2d,mid_layer=None,deepest=False,use_dropout=True):\n",
    "        super(Generator_Layer, self).__init__()\n",
    "        down_conv=nn.Conv2d(in_c,out_c,kernel_size=4,stride=2, padding=1)\n",
    "        if deepest:\n",
    "            up_convtrans=nn.ConvTranspose2d(out_c,in_c,kernel_size=4,stride=2, padding=1)\n",
    "            self.net=nn.Sequential(*([relu_layer(True),down_conv,norm_layer(out_c)]+[ResnetBlock(out_c,'reflect',norm_layer,use_dropout,False)]*2+[relu_layer(True),up_convtrans,norm_layer(in_c)]))\n",
    "        else:\n",
    "            up_convtrans=nn.ConvTranspose2d(out_c*2,in_c,kernel_size=4,stride=2, padding=1)\n",
    "            up_layers=[relu_layer(True),up_convtrans,norm_layer(in_c)]\n",
    "            self.net=nn.Sequential(*([relu_layer(True),down_conv,norm_layer(out_c)]+[mid_layer]+up_layers+([nn.Dropout()] if use_dropout else [])))\n",
    "    def forward(self,x):\n",
    "        res=self.net(x)\n",
    "        return torch.cat([x,res], 1)\n",
    "                                 \n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_c=1,out_c=1,NGF=64,depth=5):\n",
    "        super(Generator, self).__init__()\n",
    "        mid_layers=None\n",
    "        for i in range(depth):\n",
    "            inc=8 if depth-i>3 else 2**(depth-i-1)\n",
    "            outc=8 if depth-i>3 else 2**(depth-i)\n",
    "            mid_layers=Generator_Layer(NGF*inc,NGF*outc,mid_layer=mid_layers,deepest=(i==0))\n",
    "        \n",
    "        self.net=nn.Sequential(\n",
    "            *([nn.Conv2d(in_c,NGF,3,1,1),nn.BatchNorm2d(NGF),nn.LeakyReLU(0.2,True),\n",
    "               nn.Conv2d(NGF,NGF,kernel_size=4,stride=2, padding=1,bias=True)]+\n",
    "              [mid_layers]+\n",
    "              [nn.ReLU(True),nn.Conv2d(NGF*2,NGF,3,1,1),nn.BatchNorm2d(NGF),nn.ReLU(True),\n",
    "               nn.ConvTranspose2d(NGF,out_c,kernel_size=4,stride=2, padding=1),nn.Tanh()]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "AFWEczya7trw"
   },
   "outputs": [],
   "source": [
    "G_path=\"Models/G_U_Resnet\" #can also download the U-Resnet Generator model at https://drive.google.com/open?id=15NnVLJgSQmua-0DL2bf2xae4w56TDKbM\n",
    "G=LoadModel(Generator,G_path)\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "1Eu2klUe7vUR"
   },
   "outputs": [],
   "source": [
    "for i in TestSet:\n",
    "    with torch.no_grad():\n",
    "        testdata=i.unsqueeze(0).type(dtype)\n",
    "        patches=testdata*2-1\n",
    "        G_res=G(patches)\n",
    "        G_tensor=(G_res.detach()+1)/2\n",
    "        print('image:')\n",
    "        ShowImage(testdata.cpu())\n",
    "        print('generated sketch:')\n",
    "        ShowImage(G_tensor.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "Project.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
